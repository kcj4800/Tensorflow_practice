{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE/q5JgvOYiXs88O0/WcDc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcj4800/Tensorflow_practice/blob/main/Simple_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q6OJwq67is5"
      },
      "outputs": [],
      "source": [
        "# Simple RNN ( Recurrent Neural Network )\n",
        "# I went to the library to _______. 라는 문장의 마지막 단어를 맞추는 딥러닝을 하고자 할때,\n",
        "# trainX => 문장, trainY => 문장 끝에 들어갈 단어로 딥러닝 하면 된다. \n",
        "# 답러닝 모델엔 숫자만 입력 가능 하므로 문자를 모두 숫자로 변환해준다. 예) 원핫 인코딩 \n",
        "\n",
        "'''\n",
        "데이터      정수로      원핫 인코딩\n",
        "i             0        [1,0,0,0,0,0]\n",
        "went          1        [0,1,0,0,0,0]\n",
        "to            2        [0,0,1,0,0,0]\n",
        "the           3        [0,0,0,1,0,0]\n",
        "library       4        [0,0,0,0,1,0]\n",
        "to            2        [0,0,1,0,0,0]\n",
        "'''\n",
        "\n",
        "# simple RNN레이어 : Sequence 데이터 학습에 좋다. => 데이터 간의 순서가 존재할때 좋은 레이어 : 단어, 음성, 가격 예측\n",
        "# => 일반 Dense 레이어보단 단어의 순서를 지키면서 예측값을 뽑아낼 수 있다.\n",
        "'''\n",
        "i       ->  input  ->  tanh      ->  예측  ->  h1(hidden state)\n",
        "went    ->  input  ->  tanh + h1 ->  예측  ->  h2(hidden state)\n",
        "to      ->  input  ->  tanh + h2 ->  예측  ->  h3(hidden state)\n",
        "the     ->  input  ->  tanh + h3 ->  예측  ->  h4(hidden state)\n",
        "library ->  input  ->  tanh + h4 ->  예측  ->  h5(hidden state)\n",
        "to      ->  input  ->  tanh + h5 ->  예측  ->  맞다 or 아니다\n",
        "'''\n",
        "\n",
        "# RNN 적용 예시\n",
        "# 1. 하나의 인풋 vector -> Sequence 예측 가능 () => vector to sequence 모델\n",
        "# 그림을 하나 넣고 이에 대한 자막을 자동을 달아주는 모델 등(image captioning)\n",
        "'''\n",
        "인풋행렬  ->   f      ->  예측  ->  h1(hidden state)\n",
        "               f + h1 ->  예측  ->  h2(hidden state)\n",
        "               f + h2 ->  예측  ->  h3(hidden state)\n",
        "               f + h3 ->  예측  ->  h4(hidden state)\n",
        "               f + h4 ->  예측  ->  h5(hidden state)\n",
        "               f + h5 ->  예측  ->  맞다 or 아니다\n",
        "'''\n",
        "\n",
        "# 2. Sequence입력  ->  vector 예측  => sequence to vector 모델\n",
        "# 긴문장 => 결과 ( 감정 - 선플인지 악플인지)\n",
        "'''\n",
        "느그      -> f        -> 예측 -> h1\n",
        "어머니    -> f + h1   -> 예측 -> h2\n",
        "만수무강  -> f + h2   -> 예측 -> 결과도출 - 0이면 악플아님, 1이면 악플임\n",
        "'''\n",
        "\n",
        "# 3. Sequence입력  ->  Sequence예측 => sequence to vector to sequence 모델\n",
        "# 긴 문장 => 번역 => 번역된 긴 문장\n",
        "'''                                                                   행렬에서 정보를 뽑아서\n",
        "느그      -> f        -> 예측 -> h1                                  영어 sequence 뽑아주는 놈.\n",
        "어머니    -> f + h1   -> 예측 -> h2                                           <decoder>\n",
        "만수무강  -> f + h2   -> 예측 -> 다차원의 복잡한 행렬   ->   f      ->   h1  ->  예측 - your\n",
        "                                      <encoder>              f + h1 ->   h2  ->  예측 - mom\n",
        "                                  한국어 sequence를          f + h2          ->  예측 - live long  \n",
        "                            이상한 행렬로 압축시켜주는 놈.\n",
        "'''\n",
        "\n",
        "# Simple RNN의 문제 : Diminishing Gradient\n",
        "\n",
        "# I went to the library to ______? 이 문장을 RNN 했을때 앞쪽의 단어의 중요도는 갈수록 낮아지게 된다.\n",
        "# 한국어의 경우 동사가 뒤쪽에 오기때문에 영향을 덜 받을 수 있지만, 영어는 동사가 왼쪽에 오기때문에 문제가 더 크다.\n",
        "# Simple RNN 대체품 - 처음 단어들도 오래 기억할 수 있도록 - LSTM(Long Short Term Memory) 레이어\n",
        "# Input을 넣어주면 h1(hidden state : output)외에 cs(cell state : 장기기억)를 반환한다.\n",
        "\n",
        "'''\n",
        "forget gate : input_data(첫 단어) + h1 합산 -> sigmoid\n",
        "input gate :\n",
        "\n",
        "tf.keras.layers.LSTM(128, input_shape( 데이터 1개의 shape )) : \n",
        "'''\n",
        "\n",
        "# LSTM보다 간단한 레이어 : GRU\n",
        "\n"
      ]
    }
  ]
}